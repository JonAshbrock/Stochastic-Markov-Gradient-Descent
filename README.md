# Stochastic-Markov-Gradient-Descent

The lone file in this repository is a sample script showing how one may use PyTorch to run stochastic Markov gradient descent. The script is largely identical to the PyTorch tutorial except for the three lines that perform the SMGD update beginning with 'for f in my_net.parameters()'....

The same technique can be used to adapt SMGD for other neural architectures.
